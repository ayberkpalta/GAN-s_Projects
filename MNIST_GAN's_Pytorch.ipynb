{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-06T18:27:48.579874Z",
     "start_time": "2025-06-06T18:27:44.777142Z"
    }
   },
   "source": [
    "\"\"\"GAN's Generator-> create fake image\n",
    "   GAN's Discriminator->  predict fake or real image                    \"\"\"\n",
    "#purpose maximize discrimator minimize generator\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T19:12:29.880126Z",
     "start_time": "2025-06-06T19:10:04.894417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,img_dim):\n",
    "        super().__init__()\n",
    "        self.disc=nn.Sequential(\n",
    "            nn.Linear(img_dim,128), #img_dim->28x28 mnist images\n",
    "            nn.LeakyReLU(0.2), #risk of dead neurans\n",
    "            nn.Linear(128,1),\n",
    "            nn.Sigmoid() #[0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,z_dim,img_dim): #z_dim->noise dimension\n",
    "        super().__init__()\n",
    "        self.gen=nn.Sequential(\n",
    "            nn.Linear(z_dim,256),#Convert noise vector to 256 neurons\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256,img_dim),#translates to an image that is 784 pixels by 256 inches long.\n",
    "            nn.Tanh(),#[-1,1] normalize image\n",
    "         )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n",
    "\n",
    "\n",
    "#Hyperparams\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "learning_rate=3e-4\n",
    "z_dim=64\n",
    "image_dim=784#28*28*1\n",
    "batch_size=32\n",
    "num_epochs=35\n",
    "\n",
    "\n",
    "#normalize MNIST dset mean=0 std=1\n",
    "disc=Discriminator(image_dim).to(device)\n",
    "gen=Generator(z_dim,image_dim).to(device)\n",
    "fixed_noise=torch.randn(batch_size,z_dim).to(device)\n",
    "transforms=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,),(0.3081,))])#\n",
    "\n",
    "#dataLoader->batch\n",
    "dataset=datasets.MNIST(root='./data_mnist',train=True,transform=transforms,download=True)\n",
    "loader=DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
    "opt_disc=optim.Adam(disc.parameters(),lr=learning_rate)\n",
    "opt_gen=optim.Adam(gen.parameters(),lr=learning_rate)\n",
    "criterion=nn.BCELoss()\n",
    "writer_fake=SummaryWriter(f\"runs/GAN_MNIST/fake\")\n",
    "writer_real=SummaryWriter(f\"runs/GAN_MNIST/real\")\n",
    "step=0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx,(real,_) in enumerate(loader):\n",
    "        real=real.view(-1,784).to(device) #The image is made into a 28x28 → 784 flat vector.\n",
    "        batch_size=real.shape[0]\n",
    "\n",
    "        \"\"\"Train Discriminator: max log(D(real))+log1-D(G(z))\"\"\"\n",
    "        noise=torch.randn(batch_size,z_dim).to(device)\n",
    "        fake=gen(noise)#Generate fake image from random noise.\n",
    "\n",
    "        #Label real images as 1 and fake images as 0.\n",
    "        disc_real=disc(real).view(-1)\n",
    "        lossD_real=criterion(disc_real,torch.ones_like(disc_real))\n",
    "\n",
    "        disc_fake=disc(fake).view(-1)\n",
    "        lossD_fake=criterion(disc_fake,torch.zeros_like(disc_fake))\n",
    "\n",
    "        #avg the two losses\n",
    "        lossD=(lossD_real+lossD_fake)/2\n",
    "        disc.zero_grad()\n",
    "        #backprop\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "\n",
    "        \"\"\"Train Generator: min log(1-D(G(z)))->max log(D(G(z)))\"\"\"\n",
    "        #The Generator wants to fool the Discriminator → The fake images should look like “1” (real).\n",
    "        output=disc(fake).view(-1)\n",
    "        lossG=criterion(output,torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "                data = real.reshape(-1, 1, 28, 28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\n",
    "                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
    "                )\n",
    "                writer_real.add_image(\n",
    "                    \"Mnist Real Images\", img_grid_real, global_step=step\n",
    "                )\n",
    "                step += 1\n"
   ],
   "id": "6844f3b13fc79ef2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/35] Batch 0/1875                       Loss D: 0.6689, loss G: 0.6960\n",
      "Epoch [1/35] Batch 0/1875                       Loss D: 0.2933, loss G: 1.8737\n",
      "Epoch [2/35] Batch 0/1875                       Loss D: 0.1392, loss G: 2.6075\n",
      "Epoch [3/35] Batch 0/1875                       Loss D: 0.0743, loss G: 3.0826\n",
      "Epoch [4/35] Batch 0/1875                       Loss D: 0.1858, loss G: 3.9209\n",
      "Epoch [5/35] Batch 0/1875                       Loss D: 0.0401, loss G: 3.8737\n",
      "Epoch [6/35] Batch 0/1875                       Loss D: 0.1053, loss G: 4.2930\n",
      "Epoch [7/35] Batch 0/1875                       Loss D: 0.0323, loss G: 4.6096\n",
      "Epoch [8/35] Batch 0/1875                       Loss D: 0.1195, loss G: 4.8633\n",
      "Epoch [9/35] Batch 0/1875                       Loss D: 0.0612, loss G: 4.6216\n",
      "Epoch [10/35] Batch 0/1875                       Loss D: 0.0838, loss G: 5.1876\n",
      "Epoch [11/35] Batch 0/1875                       Loss D: 0.0376, loss G: 4.5221\n",
      "Epoch [12/35] Batch 0/1875                       Loss D: 0.1025, loss G: 5.1003\n",
      "Epoch [13/35] Batch 0/1875                       Loss D: 0.0106, loss G: 6.0558\n",
      "Epoch [14/35] Batch 0/1875                       Loss D: 0.0055, loss G: 5.4594\n",
      "Epoch [15/35] Batch 0/1875                       Loss D: 0.0578, loss G: 6.5830\n",
      "Epoch [16/35] Batch 0/1875                       Loss D: 0.0057, loss G: 5.9620\n",
      "Epoch [17/35] Batch 0/1875                       Loss D: 0.0223, loss G: 5.9222\n",
      "Epoch [18/35] Batch 0/1875                       Loss D: 0.0866, loss G: 6.1995\n",
      "Epoch [19/35] Batch 0/1875                       Loss D: 0.0332, loss G: 5.5765\n",
      "Epoch [20/35] Batch 0/1875                       Loss D: 0.0128, loss G: 5.7684\n",
      "Epoch [21/35] Batch 0/1875                       Loss D: 0.0080, loss G: 6.3842\n",
      "Epoch [22/35] Batch 0/1875                       Loss D: 0.0088, loss G: 5.5560\n",
      "Epoch [23/35] Batch 0/1875                       Loss D: 0.0157, loss G: 7.0593\n",
      "Epoch [24/35] Batch 0/1875                       Loss D: 0.0068, loss G: 6.4425\n",
      "Epoch [25/35] Batch 0/1875                       Loss D: 0.0040, loss G: 6.1538\n",
      "Epoch [26/35] Batch 0/1875                       Loss D: 0.0222, loss G: 6.4368\n",
      "Epoch [27/35] Batch 0/1875                       Loss D: 0.0107, loss G: 5.6285\n",
      "Epoch [28/35] Batch 0/1875                       Loss D: 0.0166, loss G: 5.4976\n",
      "Epoch [29/35] Batch 0/1875                       Loss D: 0.0020, loss G: 7.3775\n",
      "Epoch [30/35] Batch 0/1875                       Loss D: 0.0409, loss G: 6.0482\n",
      "Epoch [31/35] Batch 0/1875                       Loss D: 0.0550, loss G: 6.3866\n",
      "Epoch [32/35] Batch 0/1875                       Loss D: 0.0050, loss G: 6.6309\n",
      "Epoch [33/35] Batch 0/1875                       Loss D: 0.0028, loss G: 8.0678\n",
      "Epoch [34/35] Batch 0/1875                       Loss D: 0.0101, loss G: 7.0927\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T19:18:47.690710Z",
     "start_time": "2025-06-06T19:18:47.678481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"%load_ext tensorboard\n",
    "%tensorboard --logdir runs\"\"\""
   ],
   "id": "72105976238ad889",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%load_ext tensorboard\\n%tensorboard --logdir runs'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cc486e0e3a17d0fc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
